prabindh@prabindh-Precision-5510:~/work-2016/caffe-facialkp$ ./facialkp.sh 
I0829 23:57:40.147848  7708 caffe.cpp:190] Use CPU.
I0829 23:57:40.474757  7708 solver.cpp:48] Initializing solver from parameters: 
test_iter: 6
test_interval: 100
base_lr: 0.01
display: 100
max_iter: 2000
lr_policy: "fixed"
gamma: 0.1
momentum: 0.9
weight_decay: 0.01
stepsize: 300
snapshot: 1000
snapshot_prefix: "/home/prabindh/work-2016/caffe-facialkp/tmp"
solver_mode: CPU
net: "/home/prabindh/work-2016/caffe-facialkp/facialkp.prototxt"
I0829 23:57:40.474901  7708 solver.cpp:91] Creating training net from net file: /home/prabindh/work-2016/caffe-facialkp/facialkp.prototxt
I0829 23:57:40.475069  7708 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/prabindh/work-2016/caffe-facialkp/facialkp.prototxt
I0829 23:57:40.475159  7708 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0829 23:57:40.475241  7708 net.cpp:323] The NetState phase (0) differed from the phase (1) specified by a rule in layer data
I0829 23:57:40.475386  7708 net.cpp:52] Initializing net from parameters: 
name: "FKPReg"
state {
  phase: TRAIN
}
layer {
  name: "fkp"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "train.txt"
    batch_size: 64
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 2
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 7
    group: 2
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 5
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 5
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 5
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 4
    stride: 2
  }
}
layer {
  name: "drop0"
  type: "Dropout"
  bottom: "pool5"
  top: "pool5"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool5"
  top: "ip1"
  inner_product_param {
    num_output: 100
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 30
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu22"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0829 23:57:40.475651  7708 layer_factory.hpp:77] Creating layer fkp
I0829 23:57:40.475679  7708 net.cpp:94] Creating Layer fkp
I0829 23:57:40.475684  7708 net.cpp:409] fkp -> data
I0829 23:57:40.475724  7708 net.cpp:409] fkp -> label
I0829 23:57:40.475749  7708 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: train.txt
I0829 23:57:40.475801  7708 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I0829 23:57:40.476459  7708 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0829 23:57:40.799655  7708 net.cpp:144] Setting up fkp
I0829 23:57:40.799700  7708 net.cpp:151] Top shape: 64 1 96 96 (589824)
I0829 23:57:40.799706  7708 net.cpp:151] Top shape: 64 30 (1920)
I0829 23:57:40.799711  7708 net.cpp:159] Memory required for data: 2366976
I0829 23:57:40.799739  7708 layer_factory.hpp:77] Creating layer conv1
I0829 23:57:40.799770  7708 net.cpp:94] Creating Layer conv1
I0829 23:57:40.799795  7708 net.cpp:435] conv1 <- data
I0829 23:57:40.799808  7708 net.cpp:409] conv1 -> conv1
I0829 23:57:41.778355  7708 net.cpp:144] Setting up conv1
I0829 23:57:41.778427  7708 net.cpp:151] Top shape: 64 32 43 43 (3786752)
I0829 23:57:41.778441  7708 net.cpp:159] Memory required for data: 17513984
I0829 23:57:41.778501  7708 layer_factory.hpp:77] Creating layer relu2
I0829 23:57:41.778545  7708 net.cpp:94] Creating Layer relu2
I0829 23:57:41.778561  7708 net.cpp:435] relu2 <- conv1
I0829 23:57:41.778580  7708 net.cpp:396] relu2 -> conv1 (in-place)
I0829 23:57:41.778623  7708 net.cpp:144] Setting up relu2
I0829 23:57:41.778645  7708 net.cpp:151] Top shape: 64 32 43 43 (3786752)
I0829 23:57:41.778655  7708 net.cpp:159] Memory required for data: 32660992
I0829 23:57:41.778664  7708 layer_factory.hpp:77] Creating layer pool1
I0829 23:57:41.778682  7708 net.cpp:94] Creating Layer pool1
I0829 23:57:41.778697  7708 net.cpp:435] pool1 <- conv1
I0829 23:57:41.778714  7708 net.cpp:409] pool1 -> pool1
I0829 23:57:41.778759  7708 net.cpp:144] Setting up pool1
I0829 23:57:41.778779  7708 net.cpp:151] Top shape: 64 32 22 22 (991232)
I0829 23:57:41.778794  7708 net.cpp:159] Memory required for data: 36625920
I0829 23:57:41.778807  7708 layer_factory.hpp:77] Creating layer conv2
I0829 23:57:41.778837  7708 net.cpp:94] Creating Layer conv2
I0829 23:57:41.778851  7708 net.cpp:435] conv2 <- pool1
I0829 23:57:41.778873  7708 net.cpp:409] conv2 -> conv2
I0829 23:57:41.829759  7708 net.cpp:144] Setting up conv2
I0829 23:57:41.829845  7708 net.cpp:151] Top shape: 64 64 20 20 (1638400)
I0829 23:57:41.829859  7708 net.cpp:159] Memory required for data: 43179520
I0829 23:57:41.829900  7708 layer_factory.hpp:77] Creating layer relu2
I0829 23:57:41.829926  7708 net.cpp:94] Creating Layer relu2
I0829 23:57:41.829943  7708 net.cpp:435] relu2 <- conv2
I0829 23:57:41.829964  7708 net.cpp:396] relu2 -> conv2 (in-place)
I0829 23:57:41.829999  7708 net.cpp:144] Setting up relu2
I0829 23:57:41.830014  7708 net.cpp:151] Top shape: 64 64 20 20 (1638400)
I0829 23:57:41.830024  7708 net.cpp:159] Memory required for data: 49733120
I0829 23:57:41.830034  7708 layer_factory.hpp:77] Creating layer pool2
I0829 23:57:41.830060  7708 net.cpp:94] Creating Layer pool2
I0829 23:57:41.830072  7708 net.cpp:435] pool2 <- conv2
I0829 23:57:41.830090  7708 net.cpp:409] pool2 -> pool2
I0829 23:57:41.830118  7708 net.cpp:144] Setting up pool2
I0829 23:57:41.830139  7708 net.cpp:151] Top shape: 64 64 10 10 (409600)
I0829 23:57:41.830153  7708 net.cpp:159] Memory required for data: 51371520
I0829 23:57:41.830166  7708 layer_factory.hpp:77] Creating layer norm2
I0829 23:57:41.830193  7708 net.cpp:94] Creating Layer norm2
I0829 23:57:41.830209  7708 net.cpp:435] norm2 <- pool2
I0829 23:57:41.830226  7708 net.cpp:409] norm2 -> norm2
I0829 23:57:41.830348  7708 net.cpp:144] Setting up norm2
I0829 23:57:41.830371  7708 net.cpp:151] Top shape: 64 64 10 10 (409600)
I0829 23:57:41.830384  7708 net.cpp:159] Memory required for data: 53009920
I0829 23:57:41.830394  7708 layer_factory.hpp:77] Creating layer conv3
I0829 23:57:41.830487  7708 net.cpp:94] Creating Layer conv3
I0829 23:57:41.830505  7708 net.cpp:435] conv3 <- norm2
I0829 23:57:41.830528  7708 net.cpp:409] conv3 -> conv3
I0829 23:57:41.847663  7708 net.cpp:144] Setting up conv3
I0829 23:57:41.847714  7708 net.cpp:151] Top shape: 64 32 8 8 (131072)
I0829 23:57:41.847726  7708 net.cpp:159] Memory required for data: 53534208
I0829 23:57:41.847759  7708 layer_factory.hpp:77] Creating layer relu3
I0829 23:57:41.847781  7708 net.cpp:94] Creating Layer relu3
I0829 23:57:41.847795  7708 net.cpp:435] relu3 <- conv3
I0829 23:57:41.847820  7708 net.cpp:396] relu3 -> conv3 (in-place)
I0829 23:57:41.847846  7708 net.cpp:144] Setting up relu3
I0829 23:57:41.847859  7708 net.cpp:151] Top shape: 64 32 8 8 (131072)
I0829 23:57:41.847868  7708 net.cpp:159] Memory required for data: 54058496
I0829 23:57:41.847879  7708 layer_factory.hpp:77] Creating layer conv4
I0829 23:57:41.847904  7708 net.cpp:94] Creating Layer conv4
I0829 23:57:41.847915  7708 net.cpp:435] conv4 <- conv3
I0829 23:57:41.847934  7708 net.cpp:409] conv4 -> conv4
I0829 23:57:41.858518  7708 net.cpp:144] Setting up conv4
I0829 23:57:41.858566  7708 net.cpp:151] Top shape: 64 64 6 6 (147456)
I0829 23:57:41.858582  7708 net.cpp:159] Memory required for data: 54648320
I0829 23:57:41.858605  7708 layer_factory.hpp:77] Creating layer relu4
I0829 23:57:41.858626  7708 net.cpp:94] Creating Layer relu4
I0829 23:57:41.858639  7708 net.cpp:435] relu4 <- conv4
I0829 23:57:41.858660  7708 net.cpp:396] relu4 -> conv4 (in-place)
I0829 23:57:41.858685  7708 net.cpp:144] Setting up relu4
I0829 23:57:41.858700  7708 net.cpp:151] Top shape: 64 64 6 6 (147456)
I0829 23:57:41.858712  7708 net.cpp:159] Memory required for data: 55238144
I0829 23:57:41.858726  7708 layer_factory.hpp:77] Creating layer conv5
I0829 23:57:41.858749  7708 net.cpp:94] Creating Layer conv5
I0829 23:57:41.858762  7708 net.cpp:435] conv5 <- conv4
I0829 23:57:41.858779  7708 net.cpp:409] conv5 -> conv5
I0829 23:57:41.866411  7708 net.cpp:144] Setting up conv5
I0829 23:57:41.866457  7708 net.cpp:151] Top shape: 64 32 4 4 (32768)
I0829 23:57:41.866469  7708 net.cpp:159] Memory required for data: 55369216
I0829 23:57:41.866497  7708 layer_factory.hpp:77] Creating layer relu5
I0829 23:57:41.866519  7708 net.cpp:94] Creating Layer relu5
I0829 23:57:41.866533  7708 net.cpp:435] relu5 <- conv5
I0829 23:57:41.866550  7708 net.cpp:396] relu5 -> conv5 (in-place)
I0829 23:57:41.866575  7708 net.cpp:144] Setting up relu5
I0829 23:57:41.866592  7708 net.cpp:151] Top shape: 64 32 4 4 (32768)
I0829 23:57:41.866603  7708 net.cpp:159] Memory required for data: 55500288
I0829 23:57:41.866614  7708 layer_factory.hpp:77] Creating layer pool5
I0829 23:57:41.866631  7708 net.cpp:94] Creating Layer pool5
I0829 23:57:41.866647  7708 net.cpp:435] pool5 <- conv5
I0829 23:57:41.866662  7708 net.cpp:409] pool5 -> pool5
I0829 23:57:41.866689  7708 net.cpp:144] Setting up pool5
I0829 23:57:41.866705  7708 net.cpp:151] Top shape: 64 32 1 1 (2048)
I0829 23:57:41.866719  7708 net.cpp:159] Memory required for data: 55508480
I0829 23:57:41.866729  7708 layer_factory.hpp:77] Creating layer drop0
I0829 23:57:41.866746  7708 net.cpp:94] Creating Layer drop0
I0829 23:57:41.866758  7708 net.cpp:435] drop0 <- pool5
I0829 23:57:41.866776  7708 net.cpp:396] drop0 -> pool5 (in-place)
I0829 23:57:41.866798  7708 net.cpp:144] Setting up drop0
I0829 23:57:41.866813  7708 net.cpp:151] Top shape: 64 32 1 1 (2048)
I0829 23:57:41.866824  7708 net.cpp:159] Memory required for data: 55516672
I0829 23:57:41.866837  7708 layer_factory.hpp:77] Creating layer ip1
I0829 23:57:41.866860  7708 net.cpp:94] Creating Layer ip1
I0829 23:57:41.866873  7708 net.cpp:435] ip1 <- pool5
I0829 23:57:41.866889  7708 net.cpp:409] ip1 -> ip1
I0829 23:57:41.866935  7708 net.cpp:144] Setting up ip1
I0829 23:57:41.866950  7708 net.cpp:151] Top shape: 64 100 (6400)
I0829 23:57:41.866961  7708 net.cpp:159] Memory required for data: 55542272
I0829 23:57:41.866979  7708 layer_factory.hpp:77] Creating layer relu4
I0829 23:57:41.866997  7708 net.cpp:94] Creating Layer relu4
I0829 23:57:41.867060  7708 net.cpp:435] relu4 <- ip1
I0829 23:57:41.867079  7708 net.cpp:396] relu4 -> ip1 (in-place)
I0829 23:57:41.867097  7708 net.cpp:144] Setting up relu4
I0829 23:57:41.867115  7708 net.cpp:151] Top shape: 64 100 (6400)
I0829 23:57:41.867125  7708 net.cpp:159] Memory required for data: 55567872
I0829 23:57:41.867136  7708 layer_factory.hpp:77] Creating layer drop1
I0829 23:57:41.867151  7708 net.cpp:94] Creating Layer drop1
I0829 23:57:41.867163  7708 net.cpp:435] drop1 <- ip1
I0829 23:57:41.867178  7708 net.cpp:396] drop1 -> ip1 (in-place)
I0829 23:57:41.867197  7708 net.cpp:144] Setting up drop1
I0829 23:57:41.867209  7708 net.cpp:151] Top shape: 64 100 (6400)
I0829 23:57:41.867223  7708 net.cpp:159] Memory required for data: 55593472
I0829 23:57:41.867231  7708 layer_factory.hpp:77] Creating layer ip2
I0829 23:57:41.867247  7708 net.cpp:94] Creating Layer ip2
I0829 23:57:41.867259  7708 net.cpp:435] ip2 <- ip1
I0829 23:57:41.867278  7708 net.cpp:409] ip2 -> ip2
I0829 23:57:41.867313  7708 net.cpp:144] Setting up ip2
I0829 23:57:41.867329  7708 net.cpp:151] Top shape: 64 30 (1920)
I0829 23:57:41.867341  7708 net.cpp:159] Memory required for data: 55601152
I0829 23:57:41.867362  7708 layer_factory.hpp:77] Creating layer relu22
I0829 23:57:41.867375  7708 net.cpp:94] Creating Layer relu22
I0829 23:57:41.867388  7708 net.cpp:435] relu22 <- ip2
I0829 23:57:41.867403  7708 net.cpp:396] relu22 -> ip2 (in-place)
I0829 23:57:41.867420  7708 net.cpp:144] Setting up relu22
I0829 23:57:41.867434  7708 net.cpp:151] Top shape: 64 30 (1920)
I0829 23:57:41.867444  7708 net.cpp:159] Memory required for data: 55608832
I0829 23:57:41.867455  7708 layer_factory.hpp:77] Creating layer loss
I0829 23:57:41.867473  7708 net.cpp:94] Creating Layer loss
I0829 23:57:41.867485  7708 net.cpp:435] loss <- ip2
I0829 23:57:41.867497  7708 net.cpp:435] loss <- label
I0829 23:57:41.867516  7708 net.cpp:409] loss -> loss
I0829 23:57:41.867544  7708 net.cpp:144] Setting up loss
I0829 23:57:41.867558  7708 net.cpp:151] Top shape: (1)
I0829 23:57:41.867569  7708 net.cpp:154]     with loss weight 1
I0829 23:57:41.867606  7708 net.cpp:159] Memory required for data: 55608836
I0829 23:57:41.867620  7708 net.cpp:220] loss needs backward computation.
I0829 23:57:41.867640  7708 net.cpp:220] relu22 needs backward computation.
I0829 23:57:41.867651  7708 net.cpp:220] ip2 needs backward computation.
I0829 23:57:41.867665  7708 net.cpp:220] drop1 needs backward computation.
I0829 23:57:41.867677  7708 net.cpp:220] relu4 needs backward computation.
I0829 23:57:41.867688  7708 net.cpp:220] ip1 needs backward computation.
I0829 23:57:41.867700  7708 net.cpp:220] drop0 needs backward computation.
I0829 23:57:41.867712  7708 net.cpp:220] pool5 needs backward computation.
I0829 23:57:41.867725  7708 net.cpp:220] relu5 needs backward computation.
I0829 23:57:41.867736  7708 net.cpp:220] conv5 needs backward computation.
I0829 23:57:41.867748  7708 net.cpp:220] relu4 needs backward computation.
I0829 23:57:41.867759  7708 net.cpp:220] conv4 needs backward computation.
I0829 23:57:41.867774  7708 net.cpp:220] relu3 needs backward computation.
I0829 23:57:41.867785  7708 net.cpp:220] conv3 needs backward computation.
I0829 23:57:41.867797  7708 net.cpp:220] norm2 needs backward computation.
I0829 23:57:41.867810  7708 net.cpp:220] pool2 needs backward computation.
I0829 23:57:41.867823  7708 net.cpp:220] relu2 needs backward computation.
I0829 23:57:41.867835  7708 net.cpp:220] conv2 needs backward computation.
I0829 23:57:41.867847  7708 net.cpp:220] pool1 needs backward computation.
I0829 23:57:41.867859  7708 net.cpp:220] relu2 needs backward computation.
I0829 23:57:41.867873  7708 net.cpp:220] conv1 needs backward computation.
I0829 23:57:41.867887  7708 net.cpp:222] fkp does not need backward computation.
I0829 23:57:41.867898  7708 net.cpp:264] This network produces output loss
I0829 23:57:41.867943  7708 net.cpp:284] Network initialization done.
I0829 23:57:41.868484  7708 upgrade_proto.cpp:52] Attempting to upgrade input file specified using deprecated V1LayerParameter: /home/prabindh/work-2016/caffe-facialkp/facialkp.prototxt
I0829 23:57:41.868651  7708 upgrade_proto.cpp:60] Successfully upgraded file specified using deprecated V1LayerParameter
I0829 23:57:41.868702  7708 solver.cpp:181] Creating test net (#0) specified by net file: /home/prabindh/work-2016/caffe-facialkp/facialkp.prototxt
I0829 23:57:41.868775  7708 net.cpp:323] The NetState phase (1) differed from the phase (0) specified by a rule in layer fkp
I0829 23:57:41.869073  7708 net.cpp:52] Initializing net from parameters: 
name: "FKPReg"
state {
  phase: TEST
}
layer {
  name: "data"
  type: "HDF5Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "test.txt"
    batch_size: 100
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 32
    kernel_size: 11
    stride: 2
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 7
    group: 2
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 5
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 5
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 5
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 4
    stride: 2
  }
}
layer {
  name: "drop0"
  type: "Dropout"
  bottom: "pool5"
  top: "pool5"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool5"
  top: "ip1"
  inner_product_param {
    num_output: 100
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "drop1"
  type: "Dropout"
  bottom: "ip1"
  top: "ip1"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  inner_product_param {
    num_output: 30
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}
layer {
  name: "relu22"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}
layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0829 23:57:41.869551  7708 layer_factory.hpp:77] Creating layer data
I0829 23:57:41.869575  7708 net.cpp:94] Creating Layer data
I0829 23:57:41.869588  7708 net.cpp:409] data -> data
I0829 23:57:41.869611  7708 net.cpp:409] data -> label
I0829 23:57:41.869633  7708 hdf5_data_layer.cpp:79] Loading list of HDF5 filenames from: test.txt
I0829 23:57:41.869674  7708 hdf5_data_layer.cpp:93] Number of HDF5 files: 1
I0829 23:57:41.983588  7708 net.cpp:144] Setting up data
I0829 23:57:41.983629  7708 net.cpp:151] Top shape: 100 1 96 96 (921600)
I0829 23:57:41.983633  7708 net.cpp:151] Top shape: 100 30 (3000)
I0829 23:57:41.983649  7708 net.cpp:159] Memory required for data: 3698400
I0829 23:57:41.983656  7708 layer_factory.hpp:77] Creating layer conv1
I0829 23:57:41.983690  7708 net.cpp:94] Creating Layer conv1
I0829 23:57:41.983693  7708 net.cpp:435] conv1 <- data
I0829 23:57:41.983700  7708 net.cpp:409] conv1 -> conv1
I0829 23:57:41.996345  7708 net.cpp:144] Setting up conv1
I0829 23:57:41.996373  7708 net.cpp:151] Top shape: 100 32 43 43 (5916800)
I0829 23:57:41.996376  7708 net.cpp:159] Memory required for data: 27365600
I0829 23:57:41.996389  7708 layer_factory.hpp:77] Creating layer relu2
I0829 23:57:41.996398  7708 net.cpp:94] Creating Layer relu2
I0829 23:57:41.996404  7708 net.cpp:435] relu2 <- conv1
I0829 23:57:41.996412  7708 net.cpp:396] relu2 -> conv1 (in-place)
I0829 23:57:41.996422  7708 net.cpp:144] Setting up relu2
I0829 23:57:41.996426  7708 net.cpp:151] Top shape: 100 32 43 43 (5916800)
I0829 23:57:41.996431  7708 net.cpp:159] Memory required for data: 51032800
I0829 23:57:41.996434  7708 layer_factory.hpp:77] Creating layer pool1
I0829 23:57:41.996440  7708 net.cpp:94] Creating Layer pool1
I0829 23:57:41.996443  7708 net.cpp:435] pool1 <- conv1
I0829 23:57:41.996449  7708 net.cpp:409] pool1 -> pool1
I0829 23:57:41.996459  7708 net.cpp:144] Setting up pool1
I0829 23:57:41.996462  7708 net.cpp:151] Top shape: 100 32 22 22 (1548800)
I0829 23:57:41.996466  7708 net.cpp:159] Memory required for data: 57228000
I0829 23:57:41.996470  7708 layer_factory.hpp:77] Creating layer conv2
I0829 23:57:41.996479  7708 net.cpp:94] Creating Layer conv2
I0829 23:57:41.996484  7708 net.cpp:435] conv2 <- pool1
I0829 23:57:41.996489  7708 net.cpp:409] conv2 -> conv2
I0829 23:57:42.012689  7708 net.cpp:144] Setting up conv2
I0829 23:57:42.012708  7708 net.cpp:151] Top shape: 100 64 20 20 (2560000)
I0829 23:57:42.012712  7708 net.cpp:159] Memory required for data: 67468000
I0829 23:57:42.012720  7708 layer_factory.hpp:77] Creating layer relu2
I0829 23:57:42.012728  7708 net.cpp:94] Creating Layer relu2
I0829 23:57:42.012732  7708 net.cpp:435] relu2 <- conv2
I0829 23:57:42.012738  7708 net.cpp:396] relu2 -> conv2 (in-place)
I0829 23:57:42.012747  7708 net.cpp:144] Setting up relu2
I0829 23:57:42.012754  7708 net.cpp:151] Top shape: 100 64 20 20 (2560000)
I0829 23:57:42.012758  7708 net.cpp:159] Memory required for data: 77708000
I0829 23:57:42.012759  7708 layer_factory.hpp:77] Creating layer pool2
I0829 23:57:42.012768  7708 net.cpp:94] Creating Layer pool2
I0829 23:57:42.012771  7708 net.cpp:435] pool2 <- conv2
I0829 23:57:42.012775  7708 net.cpp:409] pool2 -> pool2
I0829 23:57:42.012784  7708 net.cpp:144] Setting up pool2
I0829 23:57:42.012787  7708 net.cpp:151] Top shape: 100 64 10 10 (640000)
I0829 23:57:42.012791  7708 net.cpp:159] Memory required for data: 80268000
I0829 23:57:42.012794  7708 layer_factory.hpp:77] Creating layer norm2
I0829 23:57:42.012800  7708 net.cpp:94] Creating Layer norm2
I0829 23:57:42.012804  7708 net.cpp:435] norm2 <- pool2
I0829 23:57:42.012809  7708 net.cpp:409] norm2 -> norm2
I0829 23:57:42.012828  7708 net.cpp:144] Setting up norm2
I0829 23:57:42.012833  7708 net.cpp:151] Top shape: 100 64 10 10 (640000)
I0829 23:57:42.012836  7708 net.cpp:159] Memory required for data: 82828000
I0829 23:57:42.012840  7708 layer_factory.hpp:77] Creating layer conv3
I0829 23:57:42.012848  7708 net.cpp:94] Creating Layer conv3
I0829 23:57:42.012851  7708 net.cpp:435] conv3 <- norm2
I0829 23:57:42.012856  7708 net.cpp:409] conv3 -> conv3
I0829 23:57:42.019178  7708 net.cpp:144] Setting up conv3
I0829 23:57:42.019196  7708 net.cpp:151] Top shape: 100 32 8 8 (204800)
I0829 23:57:42.019199  7708 net.cpp:159] Memory required for data: 83647200
I0829 23:57:42.019208  7708 layer_factory.hpp:77] Creating layer relu3
I0829 23:57:42.019215  7708 net.cpp:94] Creating Layer relu3
I0829 23:57:42.019239  7708 net.cpp:435] relu3 <- conv3
I0829 23:57:42.019245  7708 net.cpp:396] relu3 -> conv3 (in-place)
I0829 23:57:42.019253  7708 net.cpp:144] Setting up relu3
I0829 23:57:42.019258  7708 net.cpp:151] Top shape: 100 32 8 8 (204800)
I0829 23:57:42.019260  7708 net.cpp:159] Memory required for data: 84466400
I0829 23:57:42.019263  7708 layer_factory.hpp:77] Creating layer conv4
I0829 23:57:42.019269  7708 net.cpp:94] Creating Layer conv4
I0829 23:57:42.019273  7708 net.cpp:435] conv4 <- conv3
I0829 23:57:42.019278  7708 net.cpp:409] conv4 -> conv4
I0829 23:57:42.023704  7708 net.cpp:144] Setting up conv4
I0829 23:57:42.023718  7708 net.cpp:151] Top shape: 100 64 6 6 (230400)
I0829 23:57:42.023723  7708 net.cpp:159] Memory required for data: 85388000
I0829 23:57:42.023730  7708 layer_factory.hpp:77] Creating layer relu4
I0829 23:57:42.023735  7708 net.cpp:94] Creating Layer relu4
I0829 23:57:42.023738  7708 net.cpp:435] relu4 <- conv4
I0829 23:57:42.023743  7708 net.cpp:396] relu4 -> conv4 (in-place)
I0829 23:57:42.023751  7708 net.cpp:144] Setting up relu4
I0829 23:57:42.023754  7708 net.cpp:151] Top shape: 100 64 6 6 (230400)
I0829 23:57:42.023756  7708 net.cpp:159] Memory required for data: 86309600
I0829 23:57:42.023759  7708 layer_factory.hpp:77] Creating layer conv5
I0829 23:57:42.023766  7708 net.cpp:94] Creating Layer conv5
I0829 23:57:42.023769  7708 net.cpp:435] conv5 <- conv4
I0829 23:57:42.023775  7708 net.cpp:409] conv5 -> conv5
I0829 23:57:42.025905  7708 net.cpp:144] Setting up conv5
I0829 23:57:42.025919  7708 net.cpp:151] Top shape: 100 32 4 4 (51200)
I0829 23:57:42.025923  7708 net.cpp:159] Memory required for data: 86514400
I0829 23:57:42.025931  7708 layer_factory.hpp:77] Creating layer relu5
I0829 23:57:42.025938  7708 net.cpp:94] Creating Layer relu5
I0829 23:57:42.025941  7708 net.cpp:435] relu5 <- conv5
I0829 23:57:42.025945  7708 net.cpp:396] relu5 -> conv5 (in-place)
I0829 23:57:42.025952  7708 net.cpp:144] Setting up relu5
I0829 23:57:42.025957  7708 net.cpp:151] Top shape: 100 32 4 4 (51200)
I0829 23:57:42.025960  7708 net.cpp:159] Memory required for data: 86719200
I0829 23:57:42.025964  7708 layer_factory.hpp:77] Creating layer pool5
I0829 23:57:42.025969  7708 net.cpp:94] Creating Layer pool5
I0829 23:57:42.025974  7708 net.cpp:435] pool5 <- conv5
I0829 23:57:42.025977  7708 net.cpp:409] pool5 -> pool5
I0829 23:57:42.025985  7708 net.cpp:144] Setting up pool5
I0829 23:57:42.025988  7708 net.cpp:151] Top shape: 100 32 1 1 (3200)
I0829 23:57:42.025991  7708 net.cpp:159] Memory required for data: 86732000
I0829 23:57:42.025995  7708 layer_factory.hpp:77] Creating layer drop0
I0829 23:57:42.026000  7708 net.cpp:94] Creating Layer drop0
I0829 23:57:42.026003  7708 net.cpp:435] drop0 <- pool5
I0829 23:57:42.026010  7708 net.cpp:396] drop0 -> pool5 (in-place)
I0829 23:57:42.026015  7708 net.cpp:144] Setting up drop0
I0829 23:57:42.026020  7708 net.cpp:151] Top shape: 100 32 1 1 (3200)
I0829 23:57:42.026022  7708 net.cpp:159] Memory required for data: 86744800
I0829 23:57:42.026026  7708 layer_factory.hpp:77] Creating layer ip1
I0829 23:57:42.026034  7708 net.cpp:94] Creating Layer ip1
I0829 23:57:42.026038  7708 net.cpp:435] ip1 <- pool5
I0829 23:57:42.026043  7708 net.cpp:409] ip1 -> ip1
I0829 23:57:42.026057  7708 net.cpp:144] Setting up ip1
I0829 23:57:42.026062  7708 net.cpp:151] Top shape: 100 100 (10000)
I0829 23:57:42.026064  7708 net.cpp:159] Memory required for data: 86784800
I0829 23:57:42.026069  7708 layer_factory.hpp:77] Creating layer relu4
I0829 23:57:42.026075  7708 net.cpp:94] Creating Layer relu4
I0829 23:57:42.026079  7708 net.cpp:435] relu4 <- ip1
I0829 23:57:42.026083  7708 net.cpp:396] relu4 -> ip1 (in-place)
I0829 23:57:42.026089  7708 net.cpp:144] Setting up relu4
I0829 23:57:42.026094  7708 net.cpp:151] Top shape: 100 100 (10000)
I0829 23:57:42.026098  7708 net.cpp:159] Memory required for data: 86824800
I0829 23:57:42.026101  7708 layer_factory.hpp:77] Creating layer drop1
I0829 23:57:42.026105  7708 net.cpp:94] Creating Layer drop1
I0829 23:57:42.026109  7708 net.cpp:435] drop1 <- ip1
I0829 23:57:42.026126  7708 net.cpp:396] drop1 -> ip1 (in-place)
I0829 23:57:42.026132  7708 net.cpp:144] Setting up drop1
I0829 23:57:42.026137  7708 net.cpp:151] Top shape: 100 100 (10000)
I0829 23:57:42.026141  7708 net.cpp:159] Memory required for data: 86864800
I0829 23:57:42.026145  7708 layer_factory.hpp:77] Creating layer ip2
I0829 23:57:42.026149  7708 net.cpp:94] Creating Layer ip2
I0829 23:57:42.026154  7708 net.cpp:435] ip2 <- ip1
I0829 23:57:42.026160  7708 net.cpp:409] ip2 -> ip2
I0829 23:57:42.026170  7708 net.cpp:144] Setting up ip2
I0829 23:57:42.026175  7708 net.cpp:151] Top shape: 100 30 (3000)
I0829 23:57:42.026178  7708 net.cpp:159] Memory required for data: 86876800
I0829 23:57:42.026185  7708 layer_factory.hpp:77] Creating layer relu22
I0829 23:57:42.026190  7708 net.cpp:94] Creating Layer relu22
I0829 23:57:42.026192  7708 net.cpp:435] relu22 <- ip2
I0829 23:57:42.026197  7708 net.cpp:396] relu22 -> ip2 (in-place)
I0829 23:57:42.026203  7708 net.cpp:144] Setting up relu22
I0829 23:57:42.026207  7708 net.cpp:151] Top shape: 100 30 (3000)
I0829 23:57:42.026211  7708 net.cpp:159] Memory required for data: 86888800
I0829 23:57:42.026214  7708 layer_factory.hpp:77] Creating layer loss
I0829 23:57:42.026221  7708 net.cpp:94] Creating Layer loss
I0829 23:57:42.026224  7708 net.cpp:435] loss <- ip2
I0829 23:57:42.026228  7708 net.cpp:435] loss <- label
I0829 23:57:42.026234  7708 net.cpp:409] loss -> loss
I0829 23:57:42.026242  7708 net.cpp:144] Setting up loss
I0829 23:57:42.026247  7708 net.cpp:151] Top shape: (1)
I0829 23:57:42.026249  7708 net.cpp:154]     with loss weight 1
I0829 23:57:42.026258  7708 net.cpp:159] Memory required for data: 86888804
I0829 23:57:42.026263  7708 net.cpp:220] loss needs backward computation.
I0829 23:57:42.026268  7708 net.cpp:220] relu22 needs backward computation.
I0829 23:57:42.026271  7708 net.cpp:220] ip2 needs backward computation.
I0829 23:57:42.026274  7708 net.cpp:220] drop1 needs backward computation.
I0829 23:57:42.026278  7708 net.cpp:220] relu4 needs backward computation.
I0829 23:57:42.026281  7708 net.cpp:220] ip1 needs backward computation.
I0829 23:57:42.026285  7708 net.cpp:220] drop0 needs backward computation.
I0829 23:57:42.026289  7708 net.cpp:220] pool5 needs backward computation.
I0829 23:57:42.026294  7708 net.cpp:220] relu5 needs backward computation.
I0829 23:57:42.026298  7708 net.cpp:220] conv5 needs backward computation.
I0829 23:57:42.026301  7708 net.cpp:220] relu4 needs backward computation.
I0829 23:57:42.026304  7708 net.cpp:220] conv4 needs backward computation.
I0829 23:57:42.026309  7708 net.cpp:220] relu3 needs backward computation.
I0829 23:57:42.026314  7708 net.cpp:220] conv3 needs backward computation.
I0829 23:57:42.026327  7708 net.cpp:220] norm2 needs backward computation.
I0829 23:57:42.026331  7708 net.cpp:220] pool2 needs backward computation.
I0829 23:57:42.026336  7708 net.cpp:220] relu2 needs backward computation.
I0829 23:57:42.026340  7708 net.cpp:220] conv2 needs backward computation.
I0829 23:57:42.026343  7708 net.cpp:220] pool1 needs backward computation.
I0829 23:57:42.026347  7708 net.cpp:220] relu2 needs backward computation.
I0829 23:57:42.026351  7708 net.cpp:220] conv1 needs backward computation.
I0829 23:57:42.026355  7708 net.cpp:222] data does not need backward computation.
I0829 23:57:42.026360  7708 net.cpp:264] This network produces output loss
I0829 23:57:42.026372  7708 net.cpp:284] Network initialization done.
I0829 23:57:42.026439  7708 solver.cpp:60] Solver scaffolding done.
I0829 23:57:42.026478  7708 caffe.cpp:231] Starting Optimization
I0829 23:57:42.026482  7708 solver.cpp:304] Solving FKPReg
I0829 23:57:42.026485  7708 solver.cpp:305] Learning Rate Policy: fixed
I0829 23:57:42.026604  7708 solver.cpp:362] Iteration 0, Testing net (#0)
I0829 23:57:42.026610  7708 net.cpp:723] Ignoring source layer fkp
I0829 23:57:46.199496  7708 solver.cpp:429]     Test net output #0: loss = 2.97767 (* 1 = 2.97767 loss)
I0829 23:57:48.314728  7708 solver.cpp:242] Iteration 0 (0 iter/s, 6.288s/100 iter), loss = 2.88362
I0829 23:57:48.314798  7708 solver.cpp:261]     Train net output #0: loss = 2.88362 (* 1 = 2.88362 loss)
I0829 23:57:48.314843  7708 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0830 00:01:34.805652  7708 solver.cpp:362] Iteration 100, Testing net (#0)
I0830 00:01:34.805762  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:01:38.963697  7708 solver.cpp:429]     Test net output #0: loss = 0.0293202 (* 1 = 0.0293202 loss)
I0830 00:01:41.079138  7708 solver.cpp:242] Iteration 100 (0.42962 iter/s, 232.764s/100 iter), loss = 0.0143128
I0830 00:01:41.079167  7708 solver.cpp:261]     Train net output #0: loss = 0.0143128 (* 1 = 0.0143128 loss)
I0830 00:01:41.079197  7708 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I0830 00:05:08.963758  7708 solver.cpp:362] Iteration 200, Testing net (#0)
I0830 00:05:08.963987  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:05:12.831816  7708 solver.cpp:429]     Test net output #0: loss = 0.0292717 (* 1 = 0.0292717 loss)
I0830 00:05:14.907584  7708 solver.cpp:242] Iteration 200 (0.467666 iter/s, 213.828s/100 iter), loss = 0.0164031
I0830 00:05:14.907649  7708 solver.cpp:261]     Train net output #0: loss = 0.0164031 (* 1 = 0.0164031 loss)
I0830 00:05:14.907661  7708 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I0830 00:08:39.543193  7708 solver.cpp:362] Iteration 300, Testing net (#0)
I0830 00:08:39.543372  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:08:43.433598  7708 solver.cpp:429]     Test net output #0: loss = 0.0288441 (* 1 = 0.0288441 loss)
I0830 00:08:45.508523  7708 solver.cpp:242] Iteration 300 (0.474834 iter/s, 210.6s/100 iter), loss = 0.0155422
I0830 00:08:45.508553  7708 solver.cpp:261]     Train net output #0: loss = 0.0155422 (* 1 = 0.0155422 loss)
I0830 00:08:45.508584  7708 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I0830 00:12:13.547255  7708 solver.cpp:362] Iteration 400, Testing net (#0)
I0830 00:12:13.547461  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:12:17.428608  7708 solver.cpp:429]     Test net output #0: loss = 0.0302131 (* 1 = 0.0302131 loss)
I0830 00:12:19.493459  7708 solver.cpp:242] Iteration 400 (0.467325 iter/s, 213.984s/100 iter), loss = 0.015942
I0830 00:12:19.493489  7708 solver.cpp:261]     Train net output #0: loss = 0.015942 (* 1 = 0.015942 loss)
I0830 00:12:19.493520  7708 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I0830 00:15:50.288184  7708 solver.cpp:362] Iteration 500, Testing net (#0)
I0830 00:15:50.288321  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:15:54.751914  7708 solver.cpp:429]     Test net output #0: loss = 0.0295685 (* 1 = 0.0295685 loss)
I0830 00:15:56.925382  7708 solver.cpp:242] Iteration 500 (0.459916 iter/s, 217.431s/100 iter), loss = 0.0153608
I0830 00:15:56.925410  7708 solver.cpp:261]     Train net output #0: loss = 0.0153608 (* 1 = 0.0153608 loss)
I0830 00:15:56.925446  7708 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I0830 00:19:27.557548  7708 solver.cpp:362] Iteration 600, Testing net (#0)
I0830 00:19:27.557732  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:19:31.437330  7708 solver.cpp:429]     Test net output #0: loss = 0.0294486 (* 1 = 0.0294486 loss)
I0830 00:19:33.507448  7708 solver.cpp:242] Iteration 600 (0.461719 iter/s, 216.582s/100 iter), loss = 0.018493
I0830 00:19:33.507477  7708 solver.cpp:261]     Train net output #0: loss = 0.018493 (* 1 = 0.018493 loss)
I0830 00:19:33.507509  7708 sgd_solver.cpp:106] Iteration 600, lr = 0.01

I0830 00:22:56.435910  7708 solver.cpp:362] Iteration 700, Testing net (#0)
I0830 00:22:56.436111  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:23:00.288851  7708 solver.cpp:429]     Test net output #0: loss = 0.0297162 (* 1 = 0.0297162 loss)
I0830 00:23:02.351615  7708 solver.cpp:242] Iteration 700 (0.478826 iter/s, 208.844s/100 iter), loss = 0.0178556
I0830 00:23:02.351645  7708 solver.cpp:261]     Train net output #0: loss = 0.0178556 (* 1 = 0.0178556 loss)
I0830 00:23:02.351675  7708 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I0830 00:26:22.908195  7708 solver.cpp:362] Iteration 800, Testing net (#0)
I0830 00:26:22.908430  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:26:26.797963  7708 solver.cpp:429]     Test net output #0: loss = 0.0293341 (* 1 = 0.0293341 loss)
I0830 00:26:28.864545  7708 solver.cpp:242] Iteration 800 (0.484233 iter/s, 206.512s/100 iter), loss = 0.0154032
I0830 00:26:28.864578  7708 solver.cpp:261]     Train net output #0: loss = 0.0154032 (* 1 = 0.0154032 loss)
I0830 00:26:28.864609  7708 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I0830 00:29:49.428822  7708 solver.cpp:362] Iteration 900, Testing net (#0)
I0830 00:29:49.429011  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:29:53.266774  7708 solver.cpp:429]     Test net output #0: loss = 0.0295525 (* 1 = 0.0295525 loss)
I0830 00:29:55.342686  7708 solver.cpp:242] Iteration 900 (0.484313 iter/s, 206.478s/100 iter), loss = 0.0171278
I0830 00:29:55.342715  7708 solver.cpp:261]     Train net output #0: loss = 0.0171278 (* 1 = 0.0171278 loss)
I0830 00:29:55.342746  7708 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I0830 00:33:16.274369  7708 solver.cpp:479] Snapshotting to binary proto file /home/prabindh/work-2016/caffe-facialkp/tmp_iter_1000.caffemodel
I0830 00:33:16.276501  7708 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/prabindh/work-2016/caffe-facialkp/tmp_iter_1000.solverstate
I0830 00:33:16.277511  7708 solver.cpp:362] Iteration 1000, Testing net (#0)
I0830 00:33:16.277518  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:33:20.185917  7708 solver.cpp:429]     Test net output #0: loss = 0.0296378 (* 1 = 0.0296378 loss)
I0830 00:33:22.253690  7708 solver.cpp:242] Iteration 1000 (0.483302 iter/s, 206.91s/100 iter), loss = 0.0158057
I0830 00:33:22.253722  7708 solver.cpp:261]     Train net output #0: loss = 0.0158057 (* 1 = 0.0158057 loss)
I0830 00:33:22.253752  7708 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I0830 00:36:43.007319  7708 solver.cpp:362] Iteration 1100, Testing net (#0)
I0830 00:36:43.007570  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:36:46.773476  7708 solver.cpp:429]     Test net output #0: loss = 0.0290793 (* 1 = 0.0290793 loss)
I0830 00:36:48.833377  7708 solver.cpp:242] Iteration 1100 (0.484076 iter/s, 206.579s/100 iter), loss = 0.0145473
I0830 00:36:48.833426  7708 solver.cpp:261]     Train net output #0: loss = 0.0145473 (* 1 = 0.0145473 loss)
I0830 00:36:48.833439  7708 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I0830 00:40:08.668696  7708 solver.cpp:362] Iteration 1200, Testing net (#0)
I0830 00:40:08.668838  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:40:12.438351  7708 solver.cpp:429]     Test net output #0: loss = 0.0288669 (* 1 = 0.0288669 loss)
I0830 00:40:14.476299  7708 solver.cpp:242] Iteration 1200 (0.486282 iter/s, 205.642s/100 iter), loss = 0.0172019
I0830 00:40:14.476347  7708 solver.cpp:261]     Train net output #0: loss = 0.0172019 (* 1 = 0.0172019 loss)
I0830 00:40:14.476358  7708 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I0830 00:43:34.270210  7708 solver.cpp:362] Iteration 1300, Testing net (#0)
I0830 00:43:34.270354  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:43:38.073207  7708 solver.cpp:429]     Test net output #0: loss = 0.0301498 (* 1 = 0.0301498 loss)
I0830 00:43:40.152772  7708 solver.cpp:242] Iteration 1300 (0.486202 iter/s, 205.676s/100 iter), loss = 0.0160148
I0830 00:43:40.152806  7708 solver.cpp:261]     Train net output #0: loss = 0.0160148 (* 1 = 0.0160148 loss)
I0830 00:43:40.152837  7708 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I0830 00:47:00.276276  7708 solver.cpp:362] Iteration 1400, Testing net (#0)
I0830 00:47:00.276455  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:47:04.059787  7708 solver.cpp:429]     Test net output #0: loss = 0.0299356 (* 1 = 0.0299356 loss)
I0830 00:47:06.120403  7708 solver.cpp:242] Iteration 1400 (0.485515 iter/s, 205.967s/100 iter), loss = 0.0162566
I0830 00:47:06.120434  7708 solver.cpp:261]     Train net output #0: loss = 0.0162566 (* 1 = 0.0162566 loss)
I0830 00:47:06.120463  7708 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I0830 00:50:26.215785  7708 solver.cpp:362] Iteration 1500, Testing net (#0)
I0830 00:50:26.215996  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:50:29.985719  7708 solver.cpp:429]     Test net output #0: loss = 0.029189 (* 1 = 0.029189 loss)
I0830 00:50:32.076370  7708 solver.cpp:242] Iteration 1500 (0.485543 iter/s, 205.955s/100 iter), loss = 0.0165578
I0830 00:50:32.076402  7708 solver.cpp:261]     Train net output #0: loss = 0.0165578 (* 1 = 0.0165578 loss)
I0830 00:50:32.076431  7708 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I0830 00:53:51.737447  7708 solver.cpp:362] Iteration 1600, Testing net (#0)
I0830 00:53:51.737642  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:53:55.478013  7708 solver.cpp:429]     Test net output #0: loss = 0.0297191 (* 1 = 0.0297191 loss)
I0830 00:53:57.522747  7708 solver.cpp:242] Iteration 1600 (0.486746 iter/s, 205.446s/100 iter), loss = 0.0153028
I0830 00:53:57.522778  7708 solver.cpp:261]     Train net output #0: loss = 0.0153028 (* 1 = 0.0153028 loss)
I0830 00:53:57.522807  7708 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I0830 00:57:16.423291  7708 solver.cpp:362] Iteration 1700, Testing net (#0)
I0830 00:57:16.423494  7708 net.cpp:723] Ignoring source layer fkp
I0830 00:57:20.163949  7708 solver.cpp:429]     Test net output #0: loss = 0.0294437 (* 1 = 0.0294437 loss)
I0830 00:57:22.219151  7708 solver.cpp:242] Iteration 1700 (0.488529 iter/s, 204.696s/100 iter), loss = 0.0156771
I0830 00:57:22.219184  7708 solver.cpp:261]     Train net output #0: loss = 0.0156771 (* 1 = 0.0156771 loss)
I0830 00:57:22.219215  7708 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I0830 01:00:41.251680  7708 solver.cpp:362] Iteration 1800, Testing net (#0)
I0830 01:00:41.251807  7708 net.cpp:723] Ignoring source layer fkp
I0830 01:00:45.093643  7708 solver.cpp:429]     Test net output #0: loss = 0.0300871 (* 1 = 0.0300871 loss)
I0830 01:00:47.139300  7708 solver.cpp:242] Iteration 1800 (0.487995 iter/s, 204.92s/100 iter), loss = 0.0139905
I0830 01:00:47.139333  7708 solver.cpp:261]     Train net output #0: loss = 0.0139905 (* 1 = 0.0139905 loss)
I0830 01:00:47.139363  7708 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I0830 01:04:07.097627  7708 solver.cpp:362] Iteration 1900, Testing net (#0)
I0830 01:04:07.097836  7708 net.cpp:723] Ignoring source layer fkp
I0830 01:04:10.920967  7708 solver.cpp:429]     Test net output #0: loss = 0.0294853 (* 1 = 0.0294853 loss)
I0830 01:04:12.992597  7708 solver.cpp:242] Iteration 1900 (0.485784 iter/s, 205.853s/100 iter), loss = 0.0140158
I0830 01:04:12.992627  7708 solver.cpp:261]     Train net output #0: loss = 0.0140158 (* 1 = 0.0140158 loss)
I0830 01:04:12.992660  7708 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I0830 01:07:32.973583  7708 solver.cpp:479] Snapshotting to binary proto file /home/prabindh/work-2016/caffe-facialkp/tmp_iter_2000.caffemodel
I0830 01:07:32.975541  7708 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/prabindh/work-2016/caffe-facialkp/tmp_iter_2000.solverstate
I0830 01:07:34.425665  7708 solver.cpp:342] Iteration 2000, loss = 0.0140592
I0830 01:07:34.425693  7708 solver.cpp:362] Iteration 2000, Testing net (#0)
I0830 01:07:34.425699  7708 net.cpp:723] Ignoring source layer fkp
I0830 01:07:38.273797  7708 solver.cpp:429]     Test net output #0: loss = 0.0296069 (* 1 = 0.0296069 loss)
I0830 01:07:38.273818  7708 solver.cpp:347] Optimization Done.
I0830 01:07:38.273820  7708 caffe.cpp:234] Optimization Done.
F0830 01:07:38.277359  7708 gpu_memory.cpp:136] Check failed: error == cudaSuccess (29 vs. 0)  driver shutting down
*** Check failure stack trace: ***
    @     0x7f85302e85cd  google::LogMessage::Fail()
    @     0x7f85302ea433  google::LogMessage::SendToLog()
    @     0x7f85302e815b  google::LogMessage::Flush()
    @     0x7f85302eae1e  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f85308d7d52  caffe::GPUMemory::Manager::deallocate()
    @     0x7f85308dab90  boost::detail::sp_counted_impl_p<>::dispose()
    @     0x7f853092ea08  caffe::GPUMemory::MultiWorkspace::~MultiWorkspace()
    @     0x7f852ed5a35a  __cxa_finalize
    @     0x7f8530849be3  (unknown)
Aborted (core dumped)

